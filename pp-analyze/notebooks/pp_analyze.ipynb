{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main entrypoint for the PoliAnalyzer. Three main steps are supported and demonstrated in this notebook:\n",
    "\n",
    "1. Use NLP pipeline to create structural representation of privacy practices in privacy policies, and construct a knowledge graph\n",
    "  \n",
    "2. Construct actionable formal policies based on the information, such as *app policy* for [perennial semantic Data Terms of Use](https://dl.acm.org/doi/10.1145/3589334.3645631)\n",
    "  \n",
    "3. Perform reasoning to check compliance of the constructed *app policy* against a user profile indicating user's preferences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Set up\n",
    "\n",
    "Basic models and set-up steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import logging\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from copy import deepcopy\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from tqdm.auto import tqdm\n",
    "import pp_analyze\n",
    "from pp_analyze import analyze_pp, bulk_analyze_pp\n",
    "from pp_analyze import kg, dtou, utils\n",
    "from pp_analyze import statistics as stats\n",
    "from pp_analyze import hierarchy_helper as hh\n",
    "from pp_analyze import user_preference_analyze as upa\n",
    "from pp_analyze import website_compliance_evaluation as wce\n",
    "from pp_analyze.data_model import DataEntity, PurposeEntity\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# If you want to run Step 3, set this variable. This is the directory where the user personas are stored.\n",
    "# Each user persona is a directory containing all relevant RDF files encodining information about *data policy* in psDToU.\n",
    "user_persona_collection_dir = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "Run this cell to verify that you have a working set-up for Step 1 and Step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = '''\n",
    "# These companies may collect information about how you use our services over time, and combine it with similar information from other services and companies.\n",
    "# This information may be used to analyze and track data, determine the popularity of certain content, and under your online activity, among other things.\n",
    "# We are constantly collecting and updating information about the things you like or dislike, so we can provide you with more relevant data, more relevant ads, and a better user experience.\n",
    "# '''\n",
    "\n",
    "# Be sured this has recognized data entities -- #79 in evaluation\n",
    "s = '''\n",
    "Location information \\u2013 we collect information about your general location (such as city and country). For example, we may use the IP address to identify your general location. This information does not tell us where your device is precisely located. This information is sent as a normal part of internet traffic. In addition, we also collect implicit location information, which allows us to infer that you are either interested in a place or that you might be at the place \\u2013 this information does not actually tell us where your device is precisely located.\n",
    "'''\n",
    "\n",
    "# result = analyze_pp(s, override_cache={QueryCategory.DATA_CLASSIFICATION,QueryCategory.DATA_ENTITY,QueryCategory.DATA_PRACTICE})\n",
    "result = await analyze_pp(s, batch=True)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Analyze PP practices, and construct KG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze (query) PP practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "website_list = utils.get_website_list(varient='tranco', max_num=300)\n",
    "\n",
    "# website_list = ['msn.com']\n",
    "\n",
    "\n",
    "ret = await bulk_analyze_pp(website_list, override_cache=False, batch=True, max_num=100, non_breaking=True, discard_return=False)\n",
    "\n",
    "practices_ori, failed_tasks, errors = ret\n",
    "\n",
    "practices = deepcopy(practices_ori)\n",
    "\n",
    "practices_ori, failed_tasks, errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-process of identified practices\n",
    "\n",
    "Currently only one type of action is used: lifting category hierarchy.\n",
    "This is useful if you do not want to use low-level concepts in the category tree, and only use concepts in upper-levels.\n",
    "\n",
    "For example, you want `Advertising` (parent class) instead of `PersonalisedAdvertising` (a subclass, of `Advertising`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "practices = deepcopy(practices_ori)\n",
    "# practices = {k: v for i, (k, v) in enumerate(practices.items()) if i > 50 and i < 55}\n",
    "\n",
    "lift_to = [\n",
    "    'Contacts',\n",
    "    'SocialCommunication',\n",
    "    'MedicalHealth',\n",
    "    'Location',\n",
    "    'Picture',\n",
    "]\n",
    "for ws, v in practices.items():\n",
    "    changed = False\n",
    "    for p in v:\n",
    "        ichanged = p.lift(lift_to)\n",
    "        changed = changed or ichanged\n",
    "    if changed:\n",
    "        print(f'Changed: {ws}')\n",
    "\n",
    "# practices['facebook.com']\n",
    "practices.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select target for analysis below\n",
    "\n",
    "If you want to use the targeted analysis showed later in the notebook, run this cell. If you only want bulk general analysis, there is no need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# website_choice = list(website_list)[0]\n",
    "website_choice = list(practices.keys())[0]\n",
    "website_choice\n",
    "\n",
    "# website_choice = ('facebook.com', 'Facebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View statistics of identiied practices (after post-process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_practices = practices\n",
    "# target_practices = {website_choice: practices[website_choice]}\n",
    "\n",
    "# field_count = stats.calc_practice_field_count(practices[website_choice[0]])\n",
    "\n",
    "# res = stats.calc_count_stats(field_count)\n",
    "# res\n",
    "\n",
    "all_practices = [i for ws in target_practices.values() for i in ws]\n",
    "\n",
    "# stats.calc_practice_entity_count(practices[website_choice[0]])\n",
    "# for entity_type, v in stats.calc_practice_entity_count([i for ws in practices.values() for i in ws]).items():\n",
    "#     print(entity_type)\n",
    "#     for entity, count in sorted(v.items(), key=lambda x: x[1], reverse=True):\n",
    "#         print(f'  {entity}: {count}')\n",
    "\n",
    "\n",
    "def print_for_hierarchy(hierarchy, typed_node_with_count, depth=0):\n",
    "    printed = set()\n",
    "    for k, v in hierarchy.items():\n",
    "        if k in typed_node_with_count:\n",
    "            print(' ' * depth + f'{k}: {typed_node_with_count[k]}')\n",
    "            printed.add(k)\n",
    "        printed.update(print_for_hierarchy(v, typed_node_with_count, depth + 1))\n",
    "    if depth == 0:\n",
    "        for k, v in typed_node_with_count.items():\n",
    "            if k not in printed:\n",
    "                print(f'{k}: {v}')\n",
    "                printed.add(k)\n",
    "    return printed\n",
    "\n",
    "node_with_count = stats.calc_data_and_purpose_entity_count_with_hierarchy(all_practices, accumulate_to_parent=False)\n",
    "\n",
    "_ = print_for_hierarchy(hh.get_data_category_hierarchy(), node_with_count[DataEntity], 0)\n",
    "_ = print_for_hierarchy(hh.get_purpose_hierarchy(), node_with_count[PurposeEntity], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to KG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = kg.convert_to_kg(practices[website_choice[0]], website_choice[0], website_choice[1])\n",
    "pprint(g.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert all and store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_CACHE_DIR = Path(os.getenv(\"QUERY_CACHE_DIR\")) if os.getenv(\"QUERY_CACHE_DIR\") else None\n",
    "\n",
    "dump_target_dir = _CACHE_DIR / 'kg'\n",
    "\n",
    "for website in tqdm(list(practices.keys())):\n",
    "    g = kg.convert_to_kg(practices[website], website, website)\n",
    "    # turtle_str = g.serialize()\n",
    "    g.serialize(dump_target_dir / f\"{website}.ttl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convert to App Policy\n",
    "\n",
    "Note that the `dtou.convert_to_app_policy()` function constructs the KG internally for each PP. So there is no need to pass KG as a variable into it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_policy = dtou.convert_to_app_policy(practices[website_choice[0]], website_choice[0], website_choice[1])\n",
    "pprint(app_policy.to_rdf().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert all and store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_CACHE_DIR = Path(os.getenv(\"QUERY_CACHE_DIR\")) if os.getenv(\"QUERY_CACHE_DIR\") else None\n",
    "\n",
    "dump_target_dir = _CACHE_DIR / 'dtou'\n",
    "\n",
    "for website in tqdm(list(practices.keys())):\n",
    "    app_policy = dtou.convert_to_app_policy(practices[website], website, website)\n",
    "    app_policy.to_rdf().serialize(dump_target_dir / f\"{website}.ttl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze based on user profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "website_choice = 'microsoftonline.com'\n",
    "res, errs = await upa.analyze_pp_with_user_persona(website_choice, website_choice, data_practices=practices[website_choice],\n",
    "                                            user_persona_dir='/path-to/single/user-persona',\n",
    "                                            override_cache=True)\n",
    "pprint(res.serialize())\n",
    "pprint(errs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "from asyncio import Semaphore\n",
    "from tqdm.auto import tqdm\n",
    "from pp_analyze import website_compliance_evaluation as wce\n",
    "\n",
    "personas = wce.get_pesonas_under_dir(user_persona_collection_dir)\n",
    "\n",
    "# personas = [\n",
    "#     'allow-common-not-critical',\n",
    "#     'location-no-advertising',\n",
    "# ]\n",
    "\n",
    "\n",
    "def print_results_all(conflicts, errors):\n",
    "    if errors:\n",
    "        pprint(f\"There are some unexpected errors during analysis {errors}\")\n",
    "\n",
    "    websites_by_conflicts = wce.to_websites_by_num_conflicts(conflicts, practices.keys())\n",
    "\n",
    "    pprint((\"Websites with #conflicts:\", websites_by_conflicts))\n",
    "\n",
    "    for persona, res in conflicts.items():\n",
    "        print(f\"Personas: {persona}\")\n",
    "        for ws, res in res.items():\n",
    "            # print(f\"{ws:}\")\n",
    "            pprint((ws, res.serialize()), indent=2)\n",
    "\n",
    "conflicts, all_errors = await wce.analyze_personas(personas, practices, max_concurrency=6)\n",
    "print_results_all(conflicts, all_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check details of a website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(conflicts[personas[0]]['microsoftonline.com'].serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select websites for statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_equal = wce.websites_with_same_pp(practices.keys())\n",
    "pprint(ws_equal)\n",
    "\n",
    "websites_of_interest = [ws for ws in website_list if ws in practices]\n",
    "for ws_e in ws_equal:\n",
    "    for ws in ws_e[1:]:\n",
    "        websites_of_interest.remove(ws)\n",
    "websites_of_interest = websites_of_interest[:100]\n",
    "\n",
    "len(websites_of_interest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# websites_by_conflicts = wce.to_websites_by_num_conflicts(conflicts, practices.keys())\n",
    "websites_by_conflicts = wce.to_websites_by_num_conflicts(conflicts, websites_of_interest)\n",
    "personas_by_conflicts = wce.to_personas_by_num_conflicts(conflicts, personas, websites_of_interest)\n",
    "pprint(websites_by_conflicts)\n",
    "pprint(personas_by_conflicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sc_info= wce.get_segment_conflict_info(conflicts, websites_of_interest, practices)\n",
    "\n",
    "# out_dict = {}\n",
    "# for num, websites in websites_by_conflicts.items():\n",
    "#     out_dict[num] = {}\n",
    "#     for ws in websites:\n",
    "#         out_dict[num][ws] = sc_info[ws]\n",
    "# pprint(out_dict)\n",
    "\n",
    "out_dict = {}\n",
    "for num, websites in websites_by_conflicts.items():\n",
    "    out_dict[num] = {}\n",
    "    for ws in websites:\n",
    "        out_dict[num][ws] = [sc_info[ws][4] / sc_info[ws][5], sc_info[ws][4] / sc_info[ws][3], sc_info[ws][3] / sc_info[ws][5], sc_info[ws][0] / sc_info[ws][5], sc_info[ws][0] / sc_info[ws][3], sc_info[ws][0] / sc_info[ws][2] if sc_info[ws][0] else 0, sc_info[ws][2] / sc_info[ws][3]]\n",
    "        # out_dict[num][ws] = {\n",
    "        #     'practice_per_segment': sc_info[ws][4] / sc_info[ws][5],\n",
    "        #     'practice_per_valid_segment': sc_info[ws][4] / sc_info[ws][3],\n",
    "        #     'valid_segment_ratio': sc_info[ws][3] / sc_info[ws][5],\n",
    "        #     'conflict_per_segment': sc_info[ws][0] / sc_info[ws][5],\n",
    "        #     'conflict_per_valid_segment': sc_info[ws][0] / sc_info[ws][3],\n",
    "        #     'conflict_per_distinct_segment': sc_info[ws][0] / sc_info[ws][2] if sc_info[ws][0] else 0,\n",
    "        #     'conflicting_valid_segment_ratio': sc_info[ws][1] / sc_info[ws][3],\n",
    "        #     }\n",
    "\n",
    "out_dict2 = {}\n",
    "for num, v in out_dict.items():\n",
    "    out_sum = [0] * len(list(v.values())[0])\n",
    "    for ws, v2 in v.items():\n",
    "        for i in range(len(out_sum)):\n",
    "            out_sum[i] += v2[i]\n",
    "    out_sum = [i / len(v) for i in out_sum]\n",
    "    out_dict2[num] = out_sum\n",
    "# pprint(out_dict2)\n",
    "\n",
    "def print_values(out_dict2):\n",
    "    print('     practice_per_segment, practice_per_valid_segment, valid_segment_ratio, conflict_per_segment, conflict_per_valid_segment, conflict_per_distinct_segment, conflicting_valid_segment_ratio')\n",
    "    for num, v in sorted(out_dict2.items()):\n",
    "        print(f'{num:2}: ', end='')\n",
    "        for i in v:\n",
    "            print(f'{i:20.3f}, ', end='')\n",
    "        print()\n",
    "\n",
    "print_values(out_dict2)\n",
    "\n",
    "number_segments = [sc_info[ws][5] for ws in websites_of_interest]\n",
    "number_of_valid_segments = [sc_info[ws][3] for ws in websites_of_interest]\n",
    "number_of_conflicting_segments = [sc_info[ws][2] for ws in websites_of_interest]\n",
    "number_of_conflict_conflicts = [sc_info[ws][6] for ws in websites_of_interest]\n",
    "\n",
    "print(f'Number of segments: {sum(number_segments)},  Number of valid segments: {sum(number_of_valid_segments)},  Number of conflicting segments: {sum(number_of_conflicting_segments)}, Number of conflict conflicts: {sum(number_of_conflict_conflicts)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conflict_rate = wce.calc_average_conflict_rate_by_segment_of_websites(conflicts, websites_of_interest, 5, practices)\n",
    "\n",
    "organized_conflict_rate = {}\n",
    "for num, websites in websites_by_conflicts.items():\n",
    "    organized_conflict_rate[num] = {}\n",
    "    for ws in websites:\n",
    "        organized_conflict_rate[num][ws] = conflict_rate[ws]\n",
    "pprint(organized_conflict_rate)\n",
    "\n",
    "average_organized_conflict_rate = {}\n",
    "for num, websites in organized_conflict_rate.items():\n",
    "    average_organized_conflict_rate[num] = sum(websites.values()) / len(websites)\n",
    "pprint(average_organized_conflict_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of conflicts of every website\n",
    "num_conflicts = {persona: {ws: wce.get_number_of_conflicts(res) for ws, res in results.items() if ws in websites_of_interest} for persona, results in conflicts.items()}\n",
    "# pprint(num_conflicts)\n",
    "\n",
    "num_conflict_segments = {persona: {ws: wce.get_number_of_conflicting_segments(res) for ws, res in results.items() if ws in websites_of_interest} for persona, results in conflicts.items()}\n",
    "# pprint(num_conflict_segments)\n",
    "num_conflict_practices = {persona: {ws: wce.get_number_of_conflicting_practices(res, practices[ws]) for ws, res in results.items() if ws in websites_of_interest} for persona, results in conflicts.items()}\n",
    "\n",
    "num_conflict_segments_by_website = {}\n",
    "for persona, results in num_conflict_segments.items():\n",
    "    for ws in practices.keys():\n",
    "        if ws not in num_conflict_segments_by_website:\n",
    "            num_conflict_segments_by_website[ws] = {}\n",
    "        if ws in results:\n",
    "            num_conflict_segments_by_website[ws][persona] = results[ws]\n",
    "\n",
    "num_conflict_practice_by_website = {}\n",
    "for persona, results in num_conflict_practices.items():\n",
    "    for ws in practices.keys():\n",
    "        if ws not in num_conflict_practice_by_website:\n",
    "            num_conflict_practice_by_website[ws] = {}\n",
    "        if ws in results:\n",
    "            num_conflict_practice_by_website[ws][persona] = results[ws]\n",
    "\n",
    "\n",
    "websites_and_conflict_segments_by_conflicts = {}\n",
    "for count, iwebsite_list in websites_by_conflicts.items():\n",
    "    websites_and_conflict_segments_by_conflicts[count] = {}\n",
    "    for ws in iwebsite_list:\n",
    "        websites_and_conflict_segments_by_conflicts[count][ws] = num_conflict_segments_by_website[ws]\n",
    "\n",
    "pprint(websites_and_conflict_segments_by_conflicts)\n",
    "\n",
    "websites_and_conflict_practices_by_conflicts = {}\n",
    "for count, iwebsite_list in websites_by_conflicts.items():\n",
    "    websites_and_conflict_practices_by_conflicts[count] = {}\n",
    "    for ws in iwebsite_list:\n",
    "        websites_and_conflict_practices_by_conflicts[count][ws] = num_conflict_practice_by_website[ws]\n",
    "\n",
    "pprint(websites_and_conflict_practices_by_conflicts)\n",
    "\n",
    "\n",
    "num_conflict_segments_by_persona = num_conflict_segments\n",
    "\n",
    "\n",
    "persona_and_conflict_segments_by_conflicts = {}\n",
    "for count, persona_list in personas_by_conflicts.items():\n",
    "    persona_and_conflict_segments_by_conflicts[count] = {}\n",
    "    for persona in persona_list:\n",
    "        persona_and_conflict_segments_by_conflicts[count][persona] = num_conflict_segments_by_persona.get(persona, {})\n",
    "\n",
    "pprint(persona_and_conflict_segments_by_conflicts)\n",
    "# num_conflict_segments_by_persona['contact-ad-allow']\n",
    "# personas_by_conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
