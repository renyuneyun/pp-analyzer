{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pprint import pprint\n",
    "import csv\n",
    "from copy import deepcopy\n",
    "import json\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import src.annotation_utils as a_utils\n",
    "import src.llm_utils as llm_utils\n",
    "import src.message_utils as m_utils\n",
    "from src.env import (\n",
    "    BRAT_DATA_PATH,\n",
    ")\n",
    "import os\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For (segment, data_span)\n",
    "# data_entities = a_utils.load_data_entities_of_segments()\n",
    "\n",
    "# data_entities = [segment for segment in data_entities if segment['entities']]\n",
    "\n",
    "# For data span\n",
    "# all_data = m_utils.as_training_data_for_data_span_of_segment(data_entities)\n",
    "# all_data = m_utils.as_training_data_for_data_span_of_segment_1_1(data_entities)\n",
    "# For data classification\n",
    "# all_data = m_utils.as_training_data_for_data_classification_of_segment(data_entities)\n",
    "# For data classification (gradual, level 0)\n",
    "# all_data = m_utils.as_training_data_for_data_classification_of_segment_gradual(data_entities)\n",
    "\n",
    "## For (segment, sentence, data_span)\n",
    "data_entities = a_utils.load_data_entities_of_sentences()\n",
    "# For data span of sentence\n",
    "# all_data = m_utils.as_training_data_for_data_span_of_sentence(data_entities)\n",
    "# all_data = m_utils.as_training_data_for_data_span_of_sentence_1(data_entities)\n",
    "all_data = m_utils.as_training_data_for_data_span_of_sentence_only(data_entities)\n",
    "\n",
    "\n",
    "## For (sentence, purpose_span)\n",
    "# purpose_entities = a_utils.load_purpose_entities_of_sentences()\n",
    "# For purpose span of sentence\n",
    "# all_data = m_utils.as_training_data_for_purpose_span_of_sentence_only(purpose_entities)\n",
    "\n",
    "\n",
    "## For (segment, sentence, action_type, text)\n",
    "# action_entities = a_utils.load_action_of_segment()\n",
    "# For action type of sentence\n",
    "# all_data = m_utils.as_training_data_for_action_span_for_segment(action_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from a previous fine-tune job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 8, 1019)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# job_desc_dir = None\n",
    "job_desc_dir = 'fine_tune-2024-09-18-19-26-24-gpt-4o-2024-08-06'\n",
    "\n",
    "training_set, validation_set, test_set, fine_tuned_model_id = llm_utils.load_eval_info(job_desc_dir, all_data=all_data)\n",
    "\n",
    "len(training_set), len(validation_set), len(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or, Use the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(237,\n",
       " {'messages': [{'role': 'system',\n",
       "    'content': 'You are an annotation expert. You will be given a segment of a privacy policy of a web or mobile application, and will be asked to annotate actions in it.\\n\\nIMPORTANT: Filtering Out General Phrases\\nBefore annotating, carefully check each potential data entity. DO NOT annotate sentences that do not provide specific data types or purpose types.\\nExamples of general phrases to omit include, but are not limited to:\\n\\n\"the information we collect about you\"\\n\"other data\"\\n\"any information\"\\n\"other purposes\"\\n\"purposes described in our policy\"\\n\\nIf a sentence does not clearly indicate a specific type of personal data, DO NOT include it in your annotations.\\n\\nData usage context refers to the (core phrases within) sentences that mention the actions that are being taken with the PERSONAL DATA OF THE USER which is being mentioned in one of the following context types:\\n1. first-party-collection-use - the policy segment mentions collection, usage, or processing of this datum by the first party (the application).\\n2. third-party-collection-use - the policy segment mentions collection, usage, or processing of this datum by a third party.\\n3. third-party-sharing-disclosure - the policy segment mentions sharing, or disclosure of this datum to a third party.\\n4. data-storage-retention-deletion - the policy segment mentions storage, retention, or deletion of this datum.\\n5. data-security-protection - the policy segment mentions how this datum is being protected.\\n\\nNote the following!\\n1. Personal user data that is mentioned outside of one of these contexts does not classify as data entity.\\n2. Multiple contexts may apply to the same data entity.\\n3. Tracking technologies such as cookies, web beacons, etc. are technologies, hence does not classify as data entity and should NOT be annotated.\\n4. The policy segment that you receive might be empty or not contain any information on usage of concrete personal user data - that\\'s normal, and you should just return an empty list.\\n\\nProvide the output as a list of data usage contexts with the following details for each entry:\\n1. The type of the context as in one of the five context types mentioned above.\\n2. The exact text of the context as it appears in the text segment.\\n3. The exact sentence of the context as it appears in the text segment.\\n\\nYou will be given the segment below from user input. Please identify and annotate ALL data usage contexts in the following privacy policy segment.\\nYour output should follow this JSON structure:\\n\\n[\\n    {\\n        \"type\": \"...\",\\n        \"text\": \"...\",\\n        \"sentence\": \"...\"\\n    },\\n    {\\n        \"type\": \"...\",\\n        \"text\": \"...\",\\n        \"sentence\": \"...\"\\n    }\\n]\\n'},\n",
       "   {'role': 'user',\n",
       "    'content': 'Please annotate the following segment:\\n\\nTHIRD PARTY TRACKING TECHNOLOGIES\\n\\nWhen you visit or access our Services (for example when you visit our websites), we use (and authorize third parties to use) web beacons, cookies, pixels, scripts, tags and other technologies (\"Tracking Technologies\").\\nThe Tracking Technologies allow us to automatically collect information about you and your online behavior, as well as your device (for example your computer or mobile device), for different purposes, such as in order to enhance your navigation on our Services, improve our Services\\' performance and customize your experience on our Services. We also use this information to collect statistics about the usage of our Services, perform analytics, deliver content which is tailored to your interests and administer services to our Users, advertisers, publishers, customers and partners.\\nWe also allow third parties to collect information about you through Tracking Technologies. To learn more please visit our Cookie Policy.\\n\\n\\n'},\n",
       "   {'role': 'assistant',\n",
       "    'content': '{\"action_type\": \"first-party-collection-use\", \"text\": \"use\", \"sentence\": \"When you visit or access our Services (for example when you visit our websites), we use (and authorize third parties to use) web beacons, cookies, pixels, scripts, tags and other technologies (\\\\\"Tracking Technologies\\\\\").\"}'}]})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = all_data\n",
    "\n",
    "len(test_set), test_set[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ft:gpt-4o-2024-08-06:rui:data-entity-sent-data-ver2:A8tycn5w'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = fine_tuned_model_id if 'fine_tuned_model_id' in locals() else 'gpt-4o-mini-2024-07-18'\n",
    "# model_id = '4.0Ultra'\n",
    "# model_id = 'gpt-4o-2024-08-06'\n",
    "# model_id = 'ft:gpt-4o-2024-08-06:rui:test:A8rFT3EN'\n",
    "model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1019/1019 [08:02<00:00,  2.11it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('eval-2024-09-18-20-36-50-ft:gpt-4o-2024-08-06:rui:data-entity-sent-data-ver2:A8tycn5w',\n",
       " 1019)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_list = [data['messages'][:-1] for data in test_set]\n",
    "correct_outputs = [data['messages'][-1]['content'] for data in test_set]\n",
    "\n",
    "dir_name, obj_model_outputs = llm_utils.query_llm(model_id, messages_list, correct_outputs=correct_outputs,\n",
    "                                                  batch=False,\n",
    "                                                  desc='data_entity-sent_data-ver2')\n",
    "dir_name, len(obj_model_outputs)\n",
    "\n",
    "# Not using batch for some tasks because of rate limit\n",
    "# dir_name, batch_job = llm_utils.query_llm(model_id, messages_list, correct_outputs=correct_outputs,\n",
    "#                                                   batch=True,\n",
    "#                                                   desc='data_span-seg_entity-ver2')\n",
    "# dir_name, batch_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batch(id='batch_JqFQSt6gdH8BBa7BHgVYpeFH', completion_window='24h', created_at=1726578398, endpoint='/v1/chat/completions', input_file_id='file-bWlE4mEreTE1ns9Va1l1Gr69', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-2024-08-06 in organization org-B2C2pNzAq4paAOvhIYdFJlSv. Limit: 90,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1726664798, failed_at=1726578399, finalizing_at=None, in_progress_at=None, metadata={'description': 'data_span-seg_entity-ver2'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_utils.wait_for_batch_job_finish(batch_job.id)\n",
    "# llm_utils.retrieve_batch_query_result()\n",
    "# llm_utils.combine_batch_query_result()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
